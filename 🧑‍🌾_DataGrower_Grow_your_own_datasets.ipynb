{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wsanjay/Interesting_notebooks_collection/blob/main/%F0%9F%A7%91%E2%80%8D%F0%9F%8C%BE_DataGrower_Grow_your_own_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üßë‚Äçüåæ DataGrower - Grow your own datasets\n",
        "\n",
        "You need a dataset for training LLMs. So why not grow one from scratch?\n",
        "\n",
        "This notebook will help you to build a snthetic dataset from scratch.\n",
        "\n",
        "1. Define some instruction samples\n",
        "2. Add you configuration and create the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "d2_awtucaIKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA = [\n",
        "    \"What if the Beatles had never formed as a band?\",\n",
        "    \"Given that f(x) = 5x^3 - 2x + 3, find the value of f(2).\"\n",
        "]"
      ],
      "metadata": {
        "id": "YAMdCwRHUgRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "L5CKh6psHgWw"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade -qqq git+https://github.com/argilla-io/distilabel.git\n",
        "\n",
        "from distilabel.pipeline import Pipeline\n",
        "from distilabel.llms.huggingface import InferenceEndpointsLLM\n",
        "from distilabel.steps.generators.data import LoadDataFromDicts\n",
        "from distilabel.steps.tasks import TextGeneration\n",
        "from distilabel.steps.globals.huggingface import PushToHub\n",
        "from google.colab import userdata\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### üå± Specify the dataset to grow:\n",
        "\n",
        "# @markdown ü§ó The Huggingface hub dataset repo to push to\n",
        "REPO_ID = \"burtenshaw/gone_and_growned_my_own_dataset\" # @param {type:\"string\"}\n",
        "HF_TOKEN_NAME = \"HF_TOKEN\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown üí∏ The repo id of an LLM with a free Inference API\n",
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown üìñ Model Configuration\n",
        "TEMPERATURE = 0.7 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "MAX_TOKENS = 512 # @param {type:\"slider\", min:64, max:2048, step:64}\n",
        "HF_TOKEN = userdata.get(HF_TOKEN_NAME)\n",
        "# @markdown ---\n",
        "with Pipeline(\n",
        "    name=\"simple-text-generation-pipeline\",\n",
        "    description=\"A simple text generation pipeline\",\n",
        ") as pipeline:\n",
        "\n",
        "    llm = InferenceEndpointsLLM(\n",
        "        base_url=f\"https://api-inference.huggingface.co/models/{MODEL_ID}\",\n",
        "        api_key=HF_TOKEN\n",
        "    )\n",
        "\n",
        "    load_data = LoadDataFromDicts(\n",
        "        name=\"load_data\",\n",
        "        data=[\n",
        "            {\n",
        "                \"instruction\": sample,\n",
        "            } for sample in DATA\n",
        "        ],\n",
        "        batch_size=1,\n",
        "    )\n",
        "    generate_with_mistral = TextGeneration(\n",
        "        name=\"generate_with_mistral\", llm=llm\n",
        "    )\n",
        "\n",
        "    load_data.connect(generate_with_mistral)\n",
        "\n",
        "\n",
        "distiset = pipeline.run(\n",
        "    parameters={\n",
        "        \"generate_with_mistral\": {\n",
        "            \"llm\": {\n",
        "                \"generation_kwargs\": {\n",
        "                    \"temperature\": TEMPERATURE,\n",
        "                    \"max_new_tokens\": MAX_TOKENS,\n",
        "                    \"num_generations\": 2\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    },\n",
        ")\n",
        "\n",
        "distiset.push_to_hub(REPO_ID)"
      ]
    }
  ]
}